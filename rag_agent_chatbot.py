# -*- coding: utf-8 -*-
"""RAG Agent ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yTmlcP4wYwRz2oYpVCY38wx8fSOb1t1
"""

##############################################################################
# 1) INSTALL DEPENDENCIES
##############################################################################
!pip install faiss-cpu sentence-transformers scikit-learn matplotlib seaborn transformers accelerate

##############################################################################
# 2) IMPORTS
##############################################################################
import faiss
import numpy as np
import textwrap
import re
import torch

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns

##############################################################################
# 3) INITIALIZE MODELS
##############################################################################
# 3.1) Embedding model
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# 3.2) DeepSeek LLM (using 1.3B variant for better Colab compatibility)
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-1.3b-base")
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-1.3b-base",
    torch_dtype=torch.float16,
    device_map="auto"
)

##############################################################################
# 4) DEFINE DOCUMENTS
##############################################################################
documents = [
    {
        "id": "best_practice",
        "text": """Best practice color guide:
- Bedroom parents – Brown
- Bedroom girls – Pink
- Bedroom boy – Blue
- Bathroom – White
- Living room – Beige
"""
    },
    {
        "id": "house1",
        "text": """House 1:
- Bedroom parents – Black
- Bedroom girl – Yellow
- Bathroom – Green
- Living room – Beige
"""
    },
    {
        "id": "house2",
        "text": """House 2:
- Bedroom parents – Brown
- Bedroom girls – Pink
- Bedroom boy – Blue
- Bathroom – Green
- Living room – Beige
"""
    }
]
test_data = [
    {
        "query": "Is House 1 correctly painted according to best practice?",
        "label": 0
    },
    {
        "query": "Is House 2 correctly painted according to best practice?",
        "label": 0
    },
    {
        "query": "Which house follows best practice for living rooms?",
        "label": 1
    },
    {
        "query": "What color should a boy's bedroom be?",
        "label": 1
    }
]

##############################################################################
# 5) BUILD FAISS INDEX
##############################################################################
texts = [doc["text"] for doc in documents]
doc_ids = [doc["id"] for doc in documents]

embeddings = embed_model.encode(texts, convert_to_numpy=True)
embedding_dim = embeddings.shape[1]

index = faiss.IndexFlatL2(embedding_dim)
index.add(embeddings)

##############################################################################
# 6) RETRIEVAL & GENERATION FUNCTIONS
##############################################################################
def retrieve_faiss(query, model, index, doc_ids, texts, top_k=3):
    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)
    return [{
        "id": doc_ids[i],
        "text": texts[i],
        "distance": distances[0][rank]
    } for rank, i in enumerate(indices[0])]

def generate_answer(query, context):
    prompt = f"""<｜begin▁of▁sentence｜>You are a helpful design assistant. Answer using the context below.

Context:
{context}

Question: {query}

Answer:"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split("Answer:")[-1].strip()

def parse_correctness(answer):
    answer_lower = answer.lower()
    positive_terms = ['correct', 'match', 'yes', 'proper']
    negative_terms = ['incorrect', 'mismatch', 'no', 'wrong']
    positive_count = sum(term in answer_lower for term in positive_terms)
    negative_count = sum(term in answer_lower for term in negative_terms)
    return 1 if positive_count > negative_count else 0

##############################################################################
# 7) PARSING & COMPARISON LOGIC
##############################################################################
def parse_color_lines(text):
    color_map = {}
    for line in text.split("\n"):
        if line.startswith("- "):
            parts = re.split(r'–|-', line[2:], maxsplit=1)
            if len(parts) == 2:
                room = parts[0].strip().lower()
                color = parts[1].strip().lower()
                color_map[room] = color
    return color_map

def compare_houses(house1_text, house2_text, best_practice_text):
    guide_map = parse_color_lines(best_practice_text)
    h1_map = parse_color_lines(house1_text)
    h2_map = parse_color_lines(house2_text)

    report = ["Comparison of House 1 and House 2 with Best Practice:\n"]
    mismatch1 = mismatch2 = 0

    for room, ideal_color in guide_map.items():
        h1_color = h1_map.get(room, "N/A")
        h2_color = h2_map.get(room, "N/A")

        report.extend([
            f"Room: {room.title()}",
            f"  Best Practice: {ideal_color}",
            f"  House 1: {h1_color} ({'✅' if h1_color == ideal_color else '❌'})",
            f"  House 2: {h2_color} ({'✅' if h2_color == ideal_color else '❌'})\n"
        ])

        mismatch1 += h1_color != ideal_color
        mismatch2 += h2_color != ideal_color

    conclusion = (
        f"House 1 mismatches: {mismatch1} | House 2 mismatches: {mismatch2}\n"
        f"Conclusion: {'House 1' if mismatch1 < mismatch2 else 'House 2'} is closer to best practice"
        if mismatch1 != mismatch2 else "Both houses have equal mismatches"
    )
    report.append(conclusion)
    return "\n".join(report)

##############################################################################
# 8) RAG ANSWER FUNCTION
##############################################################################
def rag_answer(query, embed_model, index, doc_ids, texts, top_k=3):
    # Special case handling for comparison queries
    if "compare house" in query.lower():
        retrieved = retrieve_faiss(query, embed_model, index, doc_ids, texts, 3)
        docs = {doc['id']: doc['text'] for doc in retrieved}
        return compare_houses(
            docs.get('house1', documents[1]['text']),
            docs.get('house2', documents[2]['text']),
            docs.get('best_practice', documents[0]['text'])
        )

    # General case: Retrieve + Generate
    retrieved = retrieve_faiss(query, embed_model, index, doc_ids, texts, top_k)
    context = "\n\n".join(doc['text'] for doc in retrieved)
    return generate_answer(query, context)

##############################################################################
# 7) EVALUATION FRAMEWORK
##############################################################################
def evaluate_agent(test_data):
    y_true = []
    y_pred = []
    probs = []

    for test_case in test_data:
        answer = generate_answer(test_case["query"], "")
        prediction = parse_correctness(answer)

        y_true.append(test_case["label"])
        y_pred.append(prediction)
        probs.append(prediction)  # Simplified for demonstration

    # Calculate metrics
    metrics = {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred),
        "recall": recall_score(y_true, y_pred),
        "f1": f1_score(y_true, y_pred),
        "roc_auc": roc_auc_score(y_true, probs),
        "confusion_matrix": confusion_matrix(y_true, y_pred),
        "classification_report": classification_report(y_true, y_pred)
    }

    # Plotting
    plt.figure(figsize=(10, 4))

    # Confusion Matrix Heatmap
    plt.subplot(1, 2, 1)
    sns.heatmap(metrics["confusion_matrix"], annot=True, fmt="d", cmap="Blues",
                xticklabels=['Incorrect', 'Correct'], yticklabels=['Incorrect', 'Correct'])
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")

    # Metrics Bar Chart
    plt.subplot(1, 2, 2)
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1']
    metric_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1']]
    sns.barplot(x=metric_names, y=metric_values, palette="viridis")
    plt.ylim(0, 1)
    plt.title("Performance Metrics")

    plt.tight_layout()
    plt.show()

    # Print textual reports
    print("Classification Report:")
    print(metrics["classification_report"])

    print("\nROC AUC Score:", metrics["roc_auc"])

    return metrics

##############################################################################
# 8) INTERACTIVE DEMO
##############################################################################
def run_demo():
    print("Design Assistant initialized!\n")

    # Run evaluation first
    print("Running system evaluation...")
    evaluation_results = evaluate_agent(test_data)

    # Interactive session
    print("\nEnter your design questions (type 'exit' to quit):")
    while True:
        try:
            query = input("\nQuestion: ")
            if query.lower() in ['exit', 'quit']:
                break

            # Retrieve relevant documents
            retrieved_docs = retrieve_faiss(query, embed_model, index, doc_ids, texts)
            context = "\n\n".join([doc['text'] for doc in retrieved_docs])

            # Generate answer
            answer = generate_answer(query, context)

            # Format output
            print("\n" + "="*50)
            print(f"Query: {query}")
            print(f"Retrieved Context:\n{textwrap.fill(context, width=100)}")
            print("-"*50)
            print(f"Answer:\n{textwrap.fill(answer, width=100)}")
            print("="*50)

        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    run_demo()











